{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training \n",
    "Use this file to train networks where mu is fixed. classesmu.py is modified from its original version by Yi Hong Teoh, so that $\\mu$ is fixed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/Users/marinadrygala/Desktop/Marina/JijCalculator')\n",
    "import classesmu\n",
    "from classesmu import BatchSimulatedSpinLattice as bsslmu\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import ic_functions\n",
    "from ic_functions import chain, circle\n",
    "import neural_nets\n",
    "from neural_nets import Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "To train a network on a given dataset enter the file path for the desired file under the variable data_file, as well as the number of training examples from that file you wish to train with under the variable data_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "\"\"\"Specify the path where the data is stored. The required items from the file are then stored are variables.\"\"\"\n",
    "\n",
    "data_file = '/Users/marinadrygala/Desktop/Marina/mu_fixed/Data/Data_N=14_m=14_Epsilon=0.03_Size=29010.pickle'\n",
    "d = pickle.load(open(data_file, \"rb\"))\n",
    "ic = d['ic']\n",
    "m=d['m']\n",
    "mu=d['mu']\n",
    "N = ic.n\n",
    "test_set = d['test']\n",
    "\n",
    "\n",
    "\"\"\"The size of the training set should be specified by the parameter data_size\"\"\"\n",
    "data_size = 25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Creating a dataset for pytorch\"\"\"\n",
    "class Chain_Data(Dataset):\n",
    "    'Characterizes an ionchain dataset for PyTorch'\n",
    "    \n",
    "    def __init__(self):\n",
    "        'Initialization'\n",
    "        self.train_inputs = d['inputs'][:data_size]\n",
    "        self.train_outputs = d['normalized_outputs'][:data_size]\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.train_inputs)\n",
    " \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        return (self.train_inputs[index],self.train_outputs[index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions C'td\n",
    "Change the batch_size parameter as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "training_data = Chain_Data()\n",
    "training_loader = DataLoader(dataset=training_data, batch_size=batch_size,\n",
    "                             shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions C'td\n",
    "Specify the desired size for the hidden layer  as well as the number of epochs if training for a fixed number of epochs. Can also uncomment and modify the section below where it says 'Loading previous model for more training' to continue training on a saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Specify network architecture. The Net class is imported from neural_nets. If the model inputs are the Jijs and the\n",
    "outputs the Omegas then the sizes of the input and output layers is fixed.\"\"\"\n",
    "input_size = int(N*(N-1)/2)\n",
    "output_size = N*m\n",
    "hidden_size = 128*(N-2)\n",
    "model = Net(input_size, hidden_size, output_size, 1)\n",
    "model.double()\n",
    "\"\"\"Specify epochs parameter if training for a set number of epochs.\"\"\"\n",
    "#epochs = 50\n",
    "\n",
    "\"\"\"Specify the optimizer used as well as the model if a previously saved model will be used loaded for more training\"\"\"\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "\"\"\"Code to Load a model and continue training\"\"\"\n",
    "# path = '/home/gherkin/Marina/N=3/test_1/Epoch_32.pt'\n",
    "# model = Net(input_size, hidden_size, output_size)\n",
    "# checkpoint = torch.load(path)\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# for state in optimizer.state.values():\n",
    "#     for k, v in state.items():\n",
    "#         if isinstance(v, torch.Tensor):\n",
    "#             state[k] = v.double()\n",
    "\n",
    "\"\"\"Specify the loss function being used.\"\"\"\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "model.double()\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions C'td\n",
    "\n",
    "Option 1:\n",
    "Train for a fixed number of epochs\n",
    "\n",
    "Option 2:\n",
    "Train until the error on the test set is below a desired threshold value. Where the error on the test set is defined as the percentage difference between the true $J_{i,j}$ value and the on given by the network predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Loss: 2.5718, Test Error: 1.2098\n",
      "Epoch [2], Loss: 1.3006, Test Error: 0.9109\n",
      "Epoch [3], Loss: 0.9803, Test Error: 0.7622\n",
      "Epoch [4], Loss: 0.8632, Test Error: 0.733\n",
      "Epoch [5], Loss: 0.7986, Test Error: 0.6855\n",
      "Epoch [6], Loss: 0.7005, Test Error: 0.6265\n",
      "Epoch [7], Loss: 0.6295, Test Error: 0.6043\n",
      "Epoch [8], Loss: 0.5669, Test Error: 0.6366\n",
      "Epoch [9], Loss: 0.6275, Test Error: 0.6996\n",
      "Epoch [10], Loss: 0.5919, Test Error: 0.6082\n",
      "Epoch [11], Loss: 0.5366, Test Error: 0.5557\n",
      "Epoch [12], Loss: 0.5782, Test Error: 0.6071\n",
      "Epoch [13], Loss: 0.4403, Test Error: 0.5828\n",
      "Epoch [14], Loss: 0.4213, Test Error: 0.5361\n",
      "Epoch [15], Loss: 0.4627, Test Error: 0.5947\n",
      "Epoch [16], Loss: 0.4386, Test Error: 0.5815\n",
      "Epoch [17], Loss: 0.4179, Test Error: 0.5846\n",
      "Epoch [18], Loss: 0.4372, Test Error: 0.5247\n",
      "Epoch [19], Loss: 0.4416, Test Error: 0.4255\n",
      "Epoch [20], Loss: 0.3762, Test Error: 0.4858\n",
      "Epoch [21], Loss: 0.3817, Test Error: 0.5203\n",
      "Epoch [22], Loss: 0.3668, Test Error: 0.5171\n",
      "Epoch [23], Loss: 0.4156, Test Error: 0.5446\n",
      "Epoch [24], Loss: 0.3347, Test Error: 0.5243\n",
      "Epoch [25], Loss: 0.3511, Test Error: 0.5165\n",
      "Epoch [26], Loss: 0.4335, Test Error: 0.5656\n",
      "Epoch [27], Loss: 0.3592, Test Error: 0.5875\n",
      "Epoch [28], Loss: 0.3832, Test Error: 0.4503\n",
      "Epoch [29], Loss: 0.3816, Test Error: 0.5284\n",
      "Epoch [30], Loss: 0.3006, Test Error: 0.483\n",
      "Epoch [31], Loss: 0.2981, Test Error: 0.5321\n",
      "Epoch [32], Loss: 0.3732, Test Error: 0.5471\n",
      "Epoch [33], Loss: 0.3218, Test Error: 0.4977\n",
      "Epoch [34], Loss: 0.3451, Test Error: 0.5646\n",
      "Epoch [35], Loss: 0.3841, Test Error: 0.4376\n",
      "Epoch [36], Loss: 0.2726, Test Error: 0.4409\n",
      "Epoch [37], Loss: 0.3485, Test Error: 0.4366\n",
      "Epoch [38], Loss: 0.2673, Test Error: 0.5545\n",
      "Epoch [39], Loss: 0.2691, Test Error: 0.4457\n",
      "Epoch [40], Loss: 0.2849, Test Error: 0.5176\n",
      "Epoch [41], Loss: 0.2384, Test Error: 0.449\n",
      "Epoch [42], Loss: 0.3254, Test Error: 0.4209\n",
      "Epoch [43], Loss: 0.2631, Test Error: 0.4999\n",
      "Epoch [44], Loss: 0.2354, Test Error: 0.4562\n",
      "Epoch [45], Loss: 0.2729, Test Error: 0.4374\n",
      "Epoch [46], Loss: 0.2771, Test Error: 0.475\n",
      "Epoch [47], Loss: 0.2361, Test Error: 0.5214\n",
      "Epoch [48], Loss: 0.2636, Test Error: 0.4442\n",
      "Epoch [49], Loss: 0.2246, Test Error: 0.4391\n",
      "Epoch [50], Loss: 0.2640, Test Error: 0.4574\n",
      "Epoch [51], Loss: 0.2203, Test Error: 0.446\n",
      "Epoch [52], Loss: 0.2382, Test Error: 0.4636\n",
      "Epoch [53], Loss: 0.2576, Test Error: 0.4631\n",
      "Epoch [54], Loss: 0.2080, Test Error: 0.4423\n",
      "Epoch [55], Loss: 0.2170, Test Error: 0.4628\n",
      "Epoch [56], Loss: 0.1907, Test Error: 0.4185\n",
      "Epoch [57], Loss: 0.1894, Test Error: 0.4133\n",
      "Epoch [58], Loss: 0.1707, Test Error: 0.3981\n",
      "Epoch [59], Loss: 0.1817, Test Error: 0.3963\n",
      "Epoch [60], Loss: 0.2121, Test Error: 0.4329\n",
      "Epoch [61], Loss: 0.1874, Test Error: 0.4188\n",
      "Epoch [62], Loss: 0.1807, Test Error: 0.4199\n",
      "Epoch [63], Loss: 0.1600, Test Error: 0.4059\n",
      "Epoch [64], Loss: 0.2074, Test Error: 0.4609\n",
      "Epoch [65], Loss: 0.1684, Test Error: 0.3691\n",
      "Epoch [66], Loss: 0.1766, Test Error: 0.3552\n",
      "Epoch [67], Loss: 0.2023, Test Error: 0.3504\n",
      "Epoch [68], Loss: 0.1721, Test Error: 0.406\n",
      "Epoch [69], Loss: 0.1633, Test Error: 0.4084\n",
      "Epoch [70], Loss: 0.1970, Test Error: 0.399\n",
      "Epoch [71], Loss: 0.1815, Test Error: 0.4484\n",
      "Epoch [72], Loss: 0.1630, Test Error: 0.3825\n",
      "Epoch [73], Loss: 0.1481, Test Error: 0.3201\n",
      "Epoch [74], Loss: 0.1419, Test Error: 0.3416\n",
      "Epoch [75], Loss: 0.1461, Test Error: 0.3629\n",
      "Epoch [76], Loss: 0.1559, Test Error: 0.3415\n",
      "Epoch [77], Loss: 0.1407, Test Error: 0.3351\n",
      "Epoch [78], Loss: 0.1230, Test Error: 0.3355\n",
      "Epoch [79], Loss: 0.1296, Test Error: 0.3412\n",
      "Epoch [80], Loss: 0.1376, Test Error: 0.3456\n",
      "Epoch [81], Loss: 0.1354, Test Error: 0.3332\n",
      "Epoch [82], Loss: 0.1510, Test Error: 0.3647\n",
      "Epoch [83], Loss: 0.1223, Test Error: 0.3507\n",
      "Epoch [84], Loss: 0.1485, Test Error: 0.3845\n",
      "Epoch [85], Loss: 0.1525, Test Error: 0.4199\n",
      "Epoch [86], Loss: 0.1437, Test Error: 0.3655\n",
      "Epoch [87], Loss: 0.1205, Test Error: 0.3166\n",
      "Epoch [88], Loss: 0.1139, Test Error: 0.3691\n",
      "Epoch [89], Loss: 0.1296, Test Error: 0.3288\n",
      "Epoch [90], Loss: 0.1179, Test Error: 0.3566\n",
      "Epoch [91], Loss: 0.1290, Test Error: 0.3265\n",
      "Epoch [92], Loss: 0.1254, Test Error: 0.3584\n",
      "Epoch [93], Loss: 0.1182, Test Error: 0.3488\n",
      "Epoch [94], Loss: 0.1209, Test Error: 0.3537\n",
      "Epoch [95], Loss: 0.1116, Test Error: 0.3153\n",
      "Epoch [96], Loss: 0.1125, Test Error: 0.3137\n",
      "Epoch [97], Loss: 0.1258, Test Error: 0.3319\n",
      "Epoch [98], Loss: 0.1161, Test Error: 0.2946\n",
      "Epoch [99], Loss: 0.1223, Test Error: 0.317\n",
      "Epoch [100], Loss: 0.1110, Test Error: 0.3346\n",
      "Epoch [101], Loss: 0.1013, Test Error: 0.2705\n",
      "Epoch [102], Loss: 0.1134, Test Error: 0.3782\n",
      "Epoch [103], Loss: 0.1129, Test Error: 0.3002\n",
      "Epoch [104], Loss: 0.1126, Test Error: 0.3334\n",
      "Epoch [105], Loss: 0.1035, Test Error: 0.3022\n",
      "Epoch [106], Loss: 0.0939, Test Error: 0.3169\n",
      "Epoch [107], Loss: 0.1022, Test Error: 0.2973\n",
      "Epoch [108], Loss: 0.0882, Test Error: 0.2669\n",
      "Epoch [109], Loss: 0.0923, Test Error: 0.2796\n",
      "Epoch [110], Loss: 0.0873, Test Error: 0.3106\n",
      "Epoch [111], Loss: 0.1056, Test Error: 0.3465\n",
      "Epoch [112], Loss: 0.1117, Test Error: 0.2821\n",
      "Epoch [113], Loss: 0.0858, Test Error: 0.3017\n",
      "Epoch [114], Loss: 0.0938, Test Error: 0.2778\n",
      "Epoch [115], Loss: 0.0868, Test Error: 0.2751\n",
      "Epoch [116], Loss: 0.0904, Test Error: 0.2688\n",
      "Epoch [117], Loss: 0.0947, Test Error: 0.2607\n",
      "Epoch [118], Loss: 0.0868, Test Error: 0.2738\n",
      "Epoch [119], Loss: 0.0921, Test Error: 0.2811\n",
      "Epoch [120], Loss: 0.0770, Test Error: 0.2989\n",
      "Epoch [121], Loss: 0.0891, Test Error: 0.2673\n",
      "Epoch [122], Loss: 0.0867, Test Error: 0.291\n",
      "Epoch [123], Loss: 0.0849, Test Error: 0.3262\n",
      "Epoch [124], Loss: 0.0789, Test Error: 0.2453\n",
      "Total Time: 70071.93118095398\n"
     ]
    }
   ],
   "source": [
    "t=time.time()\n",
    "losses=[]\n",
    "errors=[]\n",
    "\n",
    "\"\"\"Option 2\"\"\"\n",
    "epoch=0\n",
    "\n",
    "Error = float('inf')\n",
    "while Error>0.25:\n",
    "    model.train()\n",
    "    for batch, (Jij, Omega) in enumerate(training_loader):\n",
    "        optimizer.zero_grad()\n",
    "        Omega_pred = model(Jij)\n",
    "        Jij_pred = bsslmu(ic, mu, Omega_pred.view(batch_size,N,m), dev=device).normalize()\n",
    "        loss = criterion(Jij, Jij_pred)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    model.eval()\n",
    "    Omega_pred = model(test_set).view(test_set.size()[0],N,m)\n",
    "    Jij_pred = bsslmu(ic, mu, Omega_pred,dev=device).normalize()\n",
    "    Error = torch.mean(torch.abs(test_set-Jij_pred))*100\n",
    "    errors.append(Error.item())\n",
    "    \n",
    "    print ('Epoch [{}], Loss: {:.4f}, Test Error: {}'.format(epoch+1, loss.item(), round(Error.item(),4)))\n",
    "    epoch+=1\n",
    "elapsed_time = time.time()-t\n",
    "print('Total Time: {}'.format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set:\n",
      " tensor([[0.2774, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.2774, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2774, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.2774, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.2774, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.2774, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.2774, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2774, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.2774, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.2774, 0.0000, 0.0000, 0.0000, 0.2774, 0.0000, 0.0000, 0.2774, 0.0000,\n",
      "         0.2774],\n",
      "        [0.2673, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.2673, 0.2673, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2673, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.2673, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.2673, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.2673, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.2673, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2673, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.2673, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.2673, 0.0000, 0.0000, 0.0000, 0.2673, 0.0000, 0.0000, 0.2673, 0.0000,\n",
      "         0.2673],\n",
      "        [0.2582, 0.2582, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.2582, 0.2582, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2582, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.2582, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.2582, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.2582, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.2582, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2582, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.2582, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.2582, 0.0000, 0.0000, 0.0000, 0.2582, 0.0000, 0.0000, 0.2582, 0.0000,\n",
      "         0.2582],\n",
      "        [0.2582, 0.0000, 0.2582, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.2582, 0.2582, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2582, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.2582, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.2582, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.2582, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.2582, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2582, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.2582, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.2582, 0.0000, 0.0000, 0.0000, 0.2582, 0.0000, 0.0000, 0.2582, 0.0000,\n",
      "         0.2582],\n",
      "        [0.2582, 0.0000, 0.0000, 0.2582, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.2582, 0.2582, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2582, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.2582, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.2582, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.2582, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.2582, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2582, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.2582, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.2582, 0.0000, 0.0000, 0.0000, 0.2582, 0.0000, 0.0000, 0.2582, 0.0000,\n",
      "         0.2582],\n",
      "        [0.2582, 0.0000, 0.0000, 0.0000, 0.2582, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.2582, 0.2582, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2582, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.2582, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.2582, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.2582, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.2582, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2582, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.2582, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.2582, 0.0000, 0.0000, 0.0000, 0.2582, 0.0000, 0.0000, 0.2582, 0.0000,\n",
      "         0.2582],\n",
      "        [0.2582, 0.0000, 0.0000, 0.0000, 0.0000, 0.2582, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.2582, 0.2582, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2582, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.2582, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.2582, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.2582, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.2582, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2582, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.2582, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.2582, 0.0000, 0.0000, 0.0000, 0.2582, 0.0000, 0.0000, 0.2582, 0.0000,\n",
      "         0.2582],\n",
      "        [0.2582, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2582, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.2582, 0.2582, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2582, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.2582, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.2582, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.2582, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.2582, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2582, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.2582, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.2582, 0.0000, 0.0000, 0.0000, 0.2582, 0.0000, 0.0000, 0.2582, 0.0000,\n",
      "         0.2582]], dtype=torch.float64)\n",
      "Jijs give by Omegas predicted for test set:\n",
      " tensor([[ 2.7327e-01,  6.1756e-03, -1.7378e-03, -4.9362e-03,  1.2349e-03,\n",
      "         -8.1603e-04,  4.0228e-03, -2.6141e-03,  2.4318e-03, -3.7948e-03,\n",
      "          3.8974e-03, -3.3502e-03,  4.5569e-03,  2.8442e-01,  3.7471e-03,\n",
      "          2.9041e-03,  4.5270e-03,  2.8131e-04,  2.1546e-03, -2.5451e-03,\n",
      "         -1.6271e-03,  3.9657e-04, -5.9539e-03, -3.4939e-03,  2.7858e-03,\n",
      "          2.7923e-01,  4.9418e-03, -7.2073e-03,  7.5483e-04,  7.0485e-04,\n",
      "          1.4826e-03, -2.7675e-03,  1.6274e-04,  2.8460e-04, -1.8651e-03,\n",
      "         -2.5800e-03,  2.7096e-01, -1.4518e-04,  1.4971e-04, -2.9560e-03,\n",
      "         -4.2294e-03, -1.0928e-03, -1.2498e-03, -2.5172e-04, -1.8296e-03,\n",
      "         -3.3939e-04,  2.7575e-01, -1.2433e-03, -1.3607e-03,  6.8180e-03,\n",
      "         -4.6020e-03,  4.0554e-03, -1.4669e-03,  8.5671e-04, -2.4043e-03,\n",
      "          2.7891e-01, -6.3708e-04, -1.1592e-05, -2.7173e-03,  2.9094e-03,\n",
      "         -6.3348e-03,  4.7336e-04,  2.4352e-03,  2.7852e-01, -1.4267e-03,\n",
      "         -3.3987e-03, -5.2558e-03, -3.9280e-04, -7.2070e-04, -3.3979e-03,\n",
      "          2.7603e-01,  2.3123e-03, -6.2775e-04,  4.2562e-03, -2.2484e-04,\n",
      "         -2.5258e-03,  2.7643e-01,  7.3663e-03, -5.5646e-03, -5.8552e-04,\n",
      "         -1.4186e-03,  2.7489e-01, -1.9180e-03, -1.7096e-03, -2.9902e-03,\n",
      "          2.7616e-01, -3.8070e-03, -2.4944e-03,  2.7760e-01,  6.4853e-04,\n",
      "          2.8174e-01],\n",
      "        [ 2.6456e-01,  4.6717e-03, -1.0794e-03, -6.7644e-03,  9.5726e-04,\n",
      "         -1.5071e-03,  2.4108e-03, -2.5088e-03,  2.3441e-03, -5.0909e-03,\n",
      "          2.6589e-03, -2.8056e-03,  2.7614e-01,  2.7317e-01,  2.0442e-03,\n",
      "          1.8891e-03,  5.3263e-03, -1.1604e-04,  2.8931e-04, -2.6035e-03,\n",
      "         -3.4334e-03, -1.7811e-03, -4.4247e-03, -5.1947e-03,  2.2172e-03,\n",
      "          2.6940e-01,  3.8028e-03, -6.3587e-03,  3.0010e-04,  1.8003e-03,\n",
      "          1.3037e-04, -1.3837e-03,  6.2040e-04,  8.4816e-04, -8.8457e-04,\n",
      "         -4.5224e-03,  2.6032e-01, -6.4703e-04,  5.2240e-04, -1.8088e-03,\n",
      "         -3.0429e-03, -2.7233e-03, -4.7308e-04, -8.3273e-04, -1.8800e-03,\n",
      "          8.5036e-04,  2.6371e-01,  7.0109e-04, -3.2918e-04,  6.6216e-03,\n",
      "         -4.1968e-03,  7.2938e-04, -3.0014e-03,  1.3372e-05, -2.2910e-03,\n",
      "          2.6766e-01,  9.1469e-04,  5.3672e-04, -3.5610e-03,  7.3059e-04,\n",
      "         -5.6499e-03, -7.7768e-04,  1.5042e-03,  2.6806e-01, -1.5669e-03,\n",
      "         -2.5991e-03, -5.4364e-03,  1.1467e-03, -3.2945e-06, -2.5879e-03,\n",
      "          2.6288e-01,  1.9300e-03, -8.6592e-04,  4.7854e-03,  1.1772e-03,\n",
      "         -1.8097e-03,  2.6503e-01,  6.4175e-03, -2.1331e-03, -1.2028e-03,\n",
      "         -1.3693e-03,  2.6720e-01, -2.8957e-03, -9.2293e-04, -2.5729e-03,\n",
      "          2.6689e-01, -3.5724e-03, -1.7598e-03,  2.6545e-01,  2.3091e-03,\n",
      "          2.6957e-01],\n",
      "        [ 2.5572e-01,  2.6167e-01,  3.0338e-05, -5.2295e-03,  1.6928e-03,\n",
      "          5.6205e-04,  3.6827e-03, -8.3963e-04,  2.0159e-03, -3.2316e-03,\n",
      "          2.2507e-03, -2.4103e-03,  2.6484e-01,  2.6344e-01,  8.5086e-04,\n",
      "          1.6795e-03,  3.5115e-03,  1.5350e-03,  1.4472e-03, -1.6132e-03,\n",
      "         -4.2221e-03, -2.9375e-03, -2.9557e-03, -2.1910e-03,  1.0737e-03,\n",
      "          2.6098e-01,  3.4686e-03, -5.2992e-03,  8.6548e-04,  1.6541e-03,\n",
      "         -3.5530e-04, -2.1280e-03, -1.9891e-04,  1.4952e-03, -2.0408e-03,\n",
      "         -5.6876e-03,  2.5267e-01, -1.9638e-03, -6.2882e-04, -6.4372e-04,\n",
      "         -1.1677e-03, -3.1603e-03, -2.2939e-03, -1.5565e-03, -2.7480e-03,\n",
      "         -8.0374e-04,  2.5518e-01,  1.6835e-03, -2.0526e-04,  7.3004e-03,\n",
      "         -2.6046e-03,  2.6399e-03, -4.4196e-03, -8.1590e-04, -2.4511e-03,\n",
      "          2.5719e-01,  1.3748e-03, -4.8460e-04, -4.1671e-03,  2.1991e-03,\n",
      "         -4.4358e-03, -1.8835e-03,  8.1077e-06,  2.5807e-01, -2.1256e-03,\n",
      "         -2.9961e-03, -3.6859e-03,  2.1258e-03, -1.1026e-03, -2.0261e-03,\n",
      "          2.5511e-01,  1.9718e-03, -5.1511e-04,  3.0176e-03,  1.1380e-03,\n",
      "         -5.0112e-04,  2.5427e-01,  8.9126e-03, -8.0776e-04, -2.5686e-04,\n",
      "         -2.2420e-03,  2.5896e-01, -2.1160e-03, -1.2744e-03, -1.3557e-04,\n",
      "          2.5631e-01,  2.9009e-04, -1.1198e-03,  2.5840e-01,  2.0714e-03,\n",
      "          2.5879e-01],\n",
      "        [ 2.5401e-01,  4.7140e-03,  2.5764e-01, -8.5747e-03,  6.4692e-04,\n",
      "         -8.5400e-04,  3.1449e-03, -3.4436e-03,  2.5648e-03, -4.0882e-03,\n",
      "          2.5690e-03, -1.7859e-03,  2.6922e-01,  2.6328e-01,  2.8307e-03,\n",
      "          1.0869e-03,  4.8135e-03, -1.2956e-03,  1.8068e-03, -4.9099e-04,\n",
      "          4.5540e-04, -1.9277e-03, -5.4863e-03, -4.8946e-03,  1.1007e-03,\n",
      "          2.5746e-01,  3.1684e-03, -7.4293e-03,  2.4523e-03,  4.5808e-04,\n",
      "          5.4057e-05, -2.0190e-03, -5.4566e-04,  1.6569e-03, -4.1458e-03,\n",
      "         -3.3719e-03,  2.5310e-01, -9.6144e-04, -1.5280e-03, -2.3123e-03,\n",
      "         -2.5046e-03, -3.5712e-03, -2.7896e-03, -4.9798e-04, -2.5155e-03,\n",
      "          1.2718e-03,  2.5476e-01,  1.0152e-03,  1.1641e-03,  6.0026e-03,\n",
      "         -3.5890e-03,  2.0610e-03, -4.0399e-03, -5.3682e-04, -2.5549e-03,\n",
      "          2.6105e-01,  1.8071e-03,  2.5865e-03, -3.2078e-03,  9.6218e-04,\n",
      "         -4.6270e-03,  9.4604e-04,  2.3536e-03,  2.5949e-01, -1.0014e-03,\n",
      "         -3.9827e-03, -5.3618e-03,  8.6228e-04,  3.9512e-05, -1.5240e-03,\n",
      "          2.5374e-01,  3.7000e-03, -1.8312e-03,  5.4399e-03,  2.2969e-03,\n",
      "         -3.4252e-03,  2.5630e-01,  5.2051e-03, -3.4357e-03,  5.9989e-04,\n",
      "         -3.6547e-05,  2.5830e-01, -9.8048e-04, -5.0845e-05, -1.8724e-03,\n",
      "          2.5774e-01, -2.1434e-03, -1.9377e-03,  2.5574e-01,  2.1779e-03,\n",
      "          2.5935e-01],\n",
      "        [ 2.5609e-01,  6.1901e-03, -3.8680e-03,  2.5408e-01, -9.4694e-04,\n",
      "         -3.1885e-03,  3.9436e-03, -3.7721e-03,  4.4261e-03, -4.4849e-03,\n",
      "          2.4957e-03, -2.1086e-03,  2.6825e-01,  2.6534e-01,  1.1883e-04,\n",
      "          3.9248e-03,  5.8462e-03,  7.2325e-04, -2.3300e-04, -3.0978e-03,\n",
      "         -6.6010e-04, -1.6971e-03, -5.7439e-03, -3.9987e-03,  3.4756e-03,\n",
      "          2.6059e-01,  4.2349e-03, -4.7213e-03,  4.5628e-04,  1.1175e-03,\n",
      "          1.5196e-03, -3.9294e-03, -4.2622e-04,  1.4185e-04, -2.9368e-04,\n",
      "         -3.5792e-03,  2.5254e-01, -1.3792e-03,  2.2571e-03, -2.1346e-03,\n",
      "         -3.3388e-03, -3.9142e-03, -1.6424e-03, -2.0695e-03, -3.0759e-03,\n",
      "         -5.3253e-04,  2.5365e-01, -3.3684e-04, -9.8561e-04,  6.7356e-03,\n",
      "         -5.6274e-03, -1.0178e-04, -5.2040e-03, -4.2003e-04, -8.5711e-04,\n",
      "          2.5768e-01,  1.1998e-03,  5.5930e-04, -3.6389e-03, -3.3913e-04,\n",
      "         -4.9537e-03, -4.4545e-04,  1.5579e-03,  2.5998e-01,  6.2753e-04,\n",
      "         -2.0308e-03, -4.9763e-03,  1.0868e-03, -1.0009e-03, -3.7064e-03,\n",
      "          2.5611e-01,  8.7830e-04, -1.4605e-03,  4.8060e-03,  2.3199e-03,\n",
      "         -1.5259e-03,  2.5499e-01,  5.6728e-03, -4.3116e-03, -1.4363e-03,\n",
      "         -1.2162e-03,  2.5825e-01, -1.3499e-03,  1.0042e-03, -9.5895e-04,\n",
      "          2.5868e-01, -2.1315e-03, -2.1606e-03,  2.5466e-01,  1.5805e-03,\n",
      "          2.6024e-01],\n",
      "        [ 2.5486e-01,  3.7102e-03, -2.9036e-03, -4.7699e-03,  2.6164e-01,\n",
      "          2.1220e-05,  3.9675e-03, -2.9418e-03,  2.5373e-03, -6.0394e-03,\n",
      "          3.7463e-03, -3.5204e-03,  2.6638e-01,  2.6409e-01,  2.6134e-03,\n",
      "          4.6933e-04,  5.0868e-03,  2.1717e-03,  1.9248e-03, -4.0541e-03,\n",
      "         -4.1886e-03, -3.5230e-03, -2.3104e-03, -4.3035e-03,  3.2937e-03,\n",
      "          2.5953e-01,  2.6406e-03, -8.0258e-03,  1.8269e-03,  6.7700e-04,\n",
      "          2.6473e-04, -4.8572e-03,  1.7312e-03, -6.1400e-04,  2.9180e-04,\n",
      "         -3.4588e-03,  2.5387e-01, -7.8865e-04,  4.0736e-04, -9.7510e-04,\n",
      "         -2.0601e-03, -2.5133e-03, -2.2260e-03, -5.3248e-06, -1.4334e-03,\n",
      "          6.4547e-04,  2.5467e-01,  1.6683e-03,  1.8003e-03,  5.3373e-03,\n",
      "         -5.5478e-03, -1.1683e-03, -3.0773e-03,  7.8428e-05, -2.8118e-03,\n",
      "          2.5690e-01,  1.2711e-03,  3.7023e-05, -4.0018e-03, -1.0040e-03,\n",
      "         -5.6354e-03, -2.0202e-03,  1.9919e-03,  2.5938e-01, -1.3176e-03,\n",
      "         -1.1553e-03, -5.6154e-03, -3.6278e-04,  6.6269e-04, -2.7386e-03,\n",
      "          2.5502e-01,  1.4630e-03, -7.9322e-04,  3.5601e-03,  5.7821e-04,\n",
      "         -1.6449e-03,  2.5634e-01,  4.9168e-03, -2.3021e-03, -1.7126e-03,\n",
      "         -1.0862e-03,  2.5667e-01, -3.4334e-03, -2.3554e-03, -2.9802e-03,\n",
      "          2.5580e-01, -6.5161e-04,  1.1530e-03,  2.5686e-01,  4.1439e-05,\n",
      "          2.5933e-01],\n",
      "        [ 2.5498e-01,  6.0852e-03, -2.7275e-03, -7.1137e-03,  7.2172e-04,\n",
      "          2.6080e-01,  4.7270e-03, -3.3590e-03,  3.1994e-03, -6.0902e-03,\n",
      "          8.2988e-04, -2.4125e-03,  2.6870e-01,  2.6414e-01,  4.8012e-04,\n",
      "          1.6631e-03,  6.4179e-03,  1.6548e-03, -1.5647e-03, -3.2275e-03,\n",
      "         -2.8982e-03, -2.5234e-03, -5.6562e-03, -3.2425e-03,  3.7116e-03,\n",
      "          2.6168e-01,  2.4813e-03, -5.7496e-03, -1.8449e-04, -1.1594e-04,\n",
      "          5.3639e-04, -4.0750e-03,  3.6153e-03, -6.2152e-04, -5.2399e-04,\n",
      "         -3.4893e-03,  2.5054e-01, -6.2892e-04,  1.3670e-03, -2.9702e-03,\n",
      "         -3.0465e-03, -4.2074e-03, -4.2665e-04, -1.6894e-03, -2.3280e-03,\n",
      "          4.0673e-04,  2.5297e-01,  9.1077e-04, -7.5637e-04,  5.4278e-03,\n",
      "         -4.0205e-03,  8.8775e-04, -2.9899e-03,  2.3120e-04, -1.1161e-03,\n",
      "          2.5623e-01, -1.1132e-03,  1.5861e-03, -6.4200e-04, -1.3303e-04,\n",
      "         -4.5911e-03, -4.2091e-04,  2.6036e-03,  2.5790e-01, -2.0849e-04,\n",
      "         -1.1666e-03, -4.5874e-03,  3.9044e-04, -2.8019e-03, -1.1591e-03,\n",
      "          2.5497e-01,  1.6865e-03, -1.9246e-03,  4.5836e-03,  2.6765e-03,\n",
      "         -1.0543e-03,  2.5795e-01,  5.5073e-03, -2.1890e-03, -1.0295e-03,\n",
      "         -1.6607e-03,  2.5635e-01, -1.7557e-03, -5.2724e-04, -1.7986e-03,\n",
      "          2.5869e-01, -2.4317e-03, -3.2547e-03,  2.5472e-01,  1.4089e-03,\n",
      "          2.6051e-01],\n",
      "        [ 2.5647e-01,  4.8938e-03, -4.1486e-03, -4.1502e-03,  2.1697e-03,\n",
      "          1.0873e-03,  2.5771e-01,  2.3793e-04,  3.3775e-03, -3.4912e-03,\n",
      "          1.9408e-03, -1.4309e-03,  2.6602e-01,  2.6342e-01,  7.0979e-04,\n",
      "          9.9851e-04,  6.6282e-03, -2.6874e-03,  2.5242e-03, -3.4168e-03,\n",
      "         -1.4643e-03, -3.0641e-03, -3.0910e-03, -5.9899e-03,  2.3713e-03,\n",
      "          2.6138e-01,  1.6564e-03, -5.9142e-03, -1.2254e-04,  8.2573e-04,\n",
      "          1.1528e-03, -9.8872e-04, -9.2386e-04,  1.9900e-03, -7.6714e-04,\n",
      "         -2.5479e-03,  2.5357e-01, -6.1193e-04, -4.2055e-04, -1.8901e-03,\n",
      "         -2.9885e-03, -1.2524e-03, -1.4174e-03, -1.1213e-03, -6.2442e-04,\n",
      "         -1.3863e-03,  2.5472e-01,  1.5826e-03, -9.1998e-04,  6.5774e-03,\n",
      "         -3.0364e-03,  1.0270e-03, -1.5873e-03,  1.3552e-04, -4.3508e-03,\n",
      "          2.5662e-01, -1.0231e-03,  1.8013e-03, -2.6166e-03, -1.5607e-03,\n",
      "         -5.7893e-03,  1.5683e-03,  3.8073e-03,  2.5581e-01, -1.6660e-03,\n",
      "         -1.8529e-03, -5.6754e-03,  4.4334e-04, -4.7953e-04, -1.6591e-03,\n",
      "          2.5699e-01,  3.4304e-03, -5.8402e-04,  3.2830e-03, -5.6312e-04,\n",
      "         -3.7295e-03,  2.5585e-01,  7.8533e-03, -2.3921e-03,  3.5719e-04,\n",
      "         -7.4883e-04,  2.5841e-01, -1.2420e-03, -4.1574e-03, -2.7923e-03,\n",
      "          2.5750e-01,  7.9741e-04, -4.7112e-04,  2.5791e-01,  3.5414e-03,\n",
      "          2.5910e-01]], dtype=torch.float64, grad_fn=<ThMulBackward>)\n",
      "Mean test set error:\n",
      " 0.24527589092751267\n"
     ]
    }
   ],
   "source": [
    "\"\"\"It is often helpful to visualize the network outputs, as well as the intented Jijs and those produced by the\n",
    "predictions. This code works for after network is trained.\"\"\"\n",
    "print('Test Set:\\n',test_set)\n",
    "print('Jijs give by Omegas predicted for test set:\\n', Jij_pred)\n",
    "print('Mean test set error:\\n', Error.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions C'td\n",
    "Input the desired file path for which to save the trained network to into the variable named saved_data_file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_data_file = '/Users/marinadrygala/Desktop/Marina/mu_fixed/non_norm/N={}/Data_Size_{}_Error_{}_Time_{}_Epochs_{}.pt'.format(N,data_size,round(Error.item(),4),round(elapsed_time,2),epoch+1)\n",
    "\n",
    "d['epoch'] = epoch\n",
    "d['model_state_dict'] = model.state_dict()\n",
    "d['optimizer_state_dict'] = optimizer.state_dict()                                                                                                                                    \n",
    "d['losses'] = losses\n",
    "d['errors'] = errors\n",
    "d['input_size'] = input_size\n",
    "d['hidden_size'] = hidden_size\n",
    "d['output_size'] = output_size\n",
    "d['Omega_restrictions'] = Omega_pred.size(1)\n",
    "\n",
    "                                                                                                                                    \n",
    "torch.save(d, saved_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
